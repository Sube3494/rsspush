"""å†…å®¹å¤„ç†å™¨æ¨¡å— - é’ˆå¯¹ä¸åŒå¹³å°çš„å†…å®¹æ ¼å¼åŒ–"""

import re
from abc import ABC, abstractmethod
from typing import Dict, Optional
from bs4 import BeautifulSoup
import html

from astrbot.api import logger


class ContentProcessor(ABC):
    """å†…å®¹å¤„ç†å™¨åŸºç±»"""
    
    @abstractmethod
    def process(self, item: dict, config: dict) -> dict:
        """å¤„ç†RSSæ¡ç›®å†…å®¹
        
        Args:
            item: åŸå§‹RSSæ¡ç›®æ•°æ®
            config: é…ç½®ä¿¡æ¯
            
        Returns:
            å¤„ç†åçš„æ•°æ®ï¼ŒåŒ…å«:
            - clean_description: æ¸…ç†åçš„æè¿°æ–‡æœ¬
            - video_url: è§†é¢‘é“¾æ¥(å¦‚æœæœ‰)
            - extra_links: å…¶ä»–é“¾æ¥(å¦‚å›¾æ–‡åœ°å€ç­‰)
            - display_title: æ˜¾ç¤ºæ ‡é¢˜(å¯èƒ½ä¸åŸæ ‡é¢˜ä¸åŒ)
        """
        pass
    
    @abstractmethod
    def match(self, url: str) -> bool:
        """åˆ¤æ–­æ­¤å¤„ç†å™¨æ˜¯å¦é€‚ç”¨äºè¯¥URL
        
        Args:
            url: RSSè®¢é˜…URL
            
        Returns:
            æ˜¯å¦åŒ¹é…
        """
        pass


class BilibiliProcessor(ContentProcessor):
    """Bç«™å†…å®¹å¤„ç†å™¨"""
    
    def match(self, url: str) -> bool:
        """åŒ¹é…Bç«™RSSé“¾æ¥"""
        return 'bilibili' in url.lower()
    
    def process(self, item: dict, config: dict) -> dict:
        """å¤„ç†Bç«™RSSå†…å®¹
        
        Bç«™RSSçš„descriptionæ ¼å¼:
        æ ¼å¼1 (è§†é¢‘åŠ¨æ€):
            æ ‡é¢˜å†…å®¹<br>-<br><img src="å°é¢å›¾"><br>è§†é¢‘åœ°å€ï¼š<a href="è§†é¢‘é“¾æ¥">...</a>
        
        æ ¼å¼2 (å›¾æ–‡åŠ¨æ€):
            æ ‡é¢˜å†…å®¹<br>åˆ†äº«å›¾ç‰‡<br><img src="å›¾ç‰‡"><br>å›¾æ–‡åœ°å€ï¼š<a href="å›¾æ–‡é“¾æ¥">...</a>
        
        æ ¼å¼3 (çº¯æ–‡æœ¬åŠ¨æ€):
            æ ‡é¢˜å†…å®¹<br>å…¶ä»–å†…å®¹...
        """
        description = item.get('description', '').strip()
        title = item.get('title', '').strip()
        
        result = {
            'clean_description': '',
            'video_url': '',
            'extra_links': {},
            'display_title': title,
        }
        
        if not description:
            return result
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(description, 'html.parser')
        
        # 1. æå–è§†é¢‘é“¾æ¥
        video_links = soup.find_all('a', href=re.compile(r'bilibili\.com/video'))
        if video_links:
            result['video_url'] = video_links[0].get('href', '')
        else:
            # å°è¯•ä»çº¯æ–‡æœ¬ä¸­æå–
            video_match = re.search(r'https://www\.bilibili\.com/video/[A-Za-z0-9]+', description)
            if video_match:
                result['video_url'] = video_match.group(0)
        
        # 2. æå–å›¾æ–‡é“¾æ¥
        opus_links = soup.find_all('a', href=re.compile(r'bilibili\.com/opus'))
        if opus_links:
            result['extra_links']['opus'] = opus_links[0].get('href', '')


        
        # 3. å¤„ç†çº¯æ–‡æœ¬ï¼ˆdescriptionå·²ç»è¢«parserè½¬æ¢ä¸ºçº¯æ–‡æœ¬ï¼‰
        text = description
        
        logger.info(f"[Bç«™å¤„ç†å™¨] åŸå§‹description: {repr(text[:300])}")
        
        # 3.1 æ ¼å¼åŒ–å„ç±»é“¾æ¥åœ°å€ï¼ˆä¿ç•™"XXåœ°å€ï¼š"å‰ç¼€ï¼Œæ·»åŠ emojiå’Œæ¢è¡Œï¼‰
        # æ ¼å¼ï¼šå—¯å—¯ - è§†é¢‘åœ°å€ï¼š https://... â†’ å—¯å—¯\nğŸ¬ è§†é¢‘åœ°å€ï¼šhttps://...
        text = re.sub(r'\s*-\s*è§†é¢‘åœ°å€[ï¼š:]\s*', '\nğŸ¬ è§†é¢‘åœ°å€ï¼š', text, flags=re.IGNORECASE)
        text = re.sub(r'\s*-\s*å›¾æ–‡åœ°å€[ï¼š:]\s*', '\nğŸ“„ å›¾æ–‡åœ°å€ï¼š', text, flags=re.IGNORECASE)
        text = re.sub(r'\s*-\s*ç›´æ’­é—´åœ°å€[ï¼š:]\s*', '\nğŸ™ï¸ ç›´æ’­é—´åœ°å€ï¼š', text, flags=re.IGNORECASE)
        # ä¹Ÿå¤„ç†æ²¡æœ‰ç ´æŠ˜å·çš„æƒ…å†µ
        text = re.sub(r'\s*è§†é¢‘åœ°å€[ï¼š:]\s*', '\nğŸ¬ è§†é¢‘åœ°å€ï¼š', text, flags=re.IGNORECASE)
        text = re.sub(r'\s*å›¾æ–‡åœ°å€[ï¼š:]\s*', '\nğŸ“„ å›¾æ–‡åœ°å€ï¼š', text, flags=re.IGNORECASE)
        text = re.sub(r'\s*ç›´æ’­é—´åœ°å€[ï¼š:]\s*', '\nğŸ™ï¸ ç›´æ’­é—´åœ°å€ï¼š', text, flags=re.IGNORECASE)
        
        logger.info(f"[Bç«™å¤„ç†å™¨] å¤„ç†åtext: {repr(text[:300])}")
        
        # 4. åˆ†è¡Œå¤„ç†
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            
            # è·³è¿‡ç©ºè¡Œ
            if not line:
                continue
            
            # è·³è¿‡å•ç‹¬çš„ç ´æŠ˜å·
            if line in ['-', 'â€”', 'â€“', 'ï¼']:
                continue
            
            # è·³è¿‡"åˆ†äº«å›¾ç‰‡"
            if line == 'åˆ†äº«å›¾ç‰‡':
                continue
            
            cleaned_lines.append(line)
        
        # 5. ç»„åˆæè¿°æ–‡æœ¬
        if cleaned_lines:
            clean_desc = '\n'.join(cleaned_lines)
            
            # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„å¼•å·
            clean_desc = re.sub(r'^["\'"]+|["\'"]+$', '', clean_desc).strip()
            
            # æˆªæ–­å¤„ç†
            max_len = config.get('push', {}).get('max_description_length', 200)
            if len(clean_desc) > max_len:
                # æ‰¾åˆ°æˆªæ–­ä½ç½®é™„è¿‘çš„æ¢è¡Œç¬¦ï¼Œé¿å…æˆªæ–­åˆ°ä¸€è¡Œä¸­é—´
                cut_pos = clean_desc.rfind('\n', 0, max_len)
                if cut_pos > max_len * 0.8:
                    clean_desc = clean_desc[:cut_pos] + '\n...'
                else:
                    clean_desc = clean_desc[:max_len] + '...'
            
            # å¦‚æœæè¿°å¤ªçŸ­ï¼Œä¸æ˜¾ç¤º
            if len(clean_desc) >= 2:
                result['clean_description'] = clean_desc
        
        logger.debug(f"Bç«™å†…å®¹å¤„ç†ç»“æœ: {result}")
        return result


class DefaultProcessor(ContentProcessor):
    """é»˜è®¤å†…å®¹å¤„ç†å™¨ - ç”¨äºæœªçŸ¥å¹³å°"""
    
    def match(self, url: str) -> bool:
        """æ€»æ˜¯è¿”å›True,ä½œä¸ºå…œåº•å¤„ç†å™¨"""
        return True
    
    def process(self, item: dict, config: dict) -> dict:
        """é»˜è®¤å¤„ç†: ç®€å•æ¸…ç†HTMLæ ‡ç­¾"""
        description = item.get('description', '').strip()
        title = item.get('title', '').strip()
        
        result = {
            'clean_description': '',
            'video_url': '',
            'extra_links': {},
            'display_title': title,
        }
        
        if not description:
            return result
        
        # ç§»é™¤HTMLæ ‡ç­¾
        soup = BeautifulSoup(description, 'html.parser')
        clean_text = soup.get_text(separator=' ', strip=True)
        
        # è§£ç HTMLå®ä½“
        clean_text = html.unescape(clean_text)
        
        # æ¸…ç†ç©ºç™½
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        # ç§»é™¤å¼•å·
        clean_text = re.sub(r'^["\'"]+|["\'"]+$', '', clean_text).strip()
        
        # æˆªæ–­
        max_len = config.get('push', {}).get('max_description_length', 200)
        if len(clean_text) > max_len:
            clean_text = clean_text[:max_len] + '...'
        
        if len(clean_text) >= 2:
            result['clean_description'] = clean_text
        
        return result


class ContentProcessorFactory:
    """å†…å®¹å¤„ç†å™¨å·¥å‚"""
    
    def __init__(self):
        # æ³¨å†Œæ‰€æœ‰å¤„ç†å™¨(é¡ºåºå¾ˆé‡è¦,DefaultProcessorå¿…é¡»æœ€å)
        self.processors = [
            BilibiliProcessor(),
            # æœªæ¥å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šå¤„ç†å™¨:
            # TwitterProcessor(),
            # WeiboProcessor(),
            # YouTubeProcessor(),
            DefaultProcessor(),  # å¿…é¡»æ”¾åœ¨æœ€åä½œä¸ºå…œåº•
        ]
    
    def get_processor(self, url: str) -> ContentProcessor:
        """æ ¹æ®URLè·å–åˆé€‚çš„å¤„ç†å™¨
        
        Args:
            url: RSSè®¢é˜…URL
            
        Returns:
            åŒ¹é…çš„å¤„ç†å™¨å®ä¾‹
        """
        for processor in self.processors:
            if processor.match(url):
                logger.debug(f"ä½¿ç”¨å¤„ç†å™¨: {processor.__class__.__name__}")
                return processor
        
        # ç†è®ºä¸Šä¸ä¼šåˆ°è¿™é‡Œ,å› ä¸ºDefaultProcessoræ€»æ˜¯åŒ¹é…
        return DefaultProcessor()
